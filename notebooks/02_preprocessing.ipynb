{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6022308f-3917-40a4-bfa7-19220d14652b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.mlflow_utils import configure_mlflow, load_config, find_latest_run_id_by_experiment_and_stage, get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130f99b1-9c44-4364-9dc3-ddaf0c25a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71da8db-edf3-4907-9495-6edeb68ab765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data_artifacts(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Log comprehensive data artifacts with schema\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # Log raw dataset sample\n",
    "        sample_path = os.path.join(tmp_dir, \"data_sample.csv\")\n",
    "        data.sample(1000).to_csv(sample_path, index=False)\n",
    "        mlflow.log_artifact(sample_path)\n",
    "        \n",
    "        # Log dataset schema\n",
    "        schema = pd.io.json.build_table_schema(data)\n",
    "        schema_path = os.path.join(tmp_dir, \"data_schema.json\")\n",
    "        with open(schema_path, \"w\") as outfile: \n",
    "            json.dump(schema, outfile, indent=2)\n",
    "            \n",
    "        mlflow.log_artifact(schema_path)\n",
    "\n",
    "def log_split_metrics(datasets: dict) -> None:\n",
    "    \"\"\"Log dataset metrics with provenance tracking\"\"\"\n",
    "    metrics = {}\n",
    "    for name, (data, _, _) in datasets.items():\n",
    "        metrics[f\"{name}_samples\"] = data.shape[0]\n",
    "        if len(data.shape) > 1:\n",
    "            metrics[f\"{name}_features\"] = data.shape[1]\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "def log_processed_data(datasets: dict, base_path: str) -> None:\n",
    "    \"\"\"Save and log processed datasets in efficient format\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        for name, (data, _, split) in datasets.items():\n",
    "            file_path = os.path.join(tmp_dir, f\"{name}.parquet\")\n",
    "            pd.DataFrame(data).to_parquet(file_path)\n",
    "            mlflow.log_artifact(file_path, os.path.join(base_path, split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6327466d-acbf-4665-803b-1986d3708720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(X_train, X_val, X_test, y_train, y_val, y_test) -> dict:\n",
    "    \"\"\"Create dataset dictionary with proper typing and metadata\"\"\"\n",
    "    return {\n",
    "        'X_train': (X_train, 'Features', 'training'),\n",
    "        'X_val': (X_val, 'Features', 'validation'),\n",
    "        'X_test': (X_test, 'Features', 'testing'),\n",
    "        'y_train': (y_train, 'Target', 'training'),\n",
    "        'y_val': (y_val, 'Target', 'validation'),\n",
    "        'y_test': (y_test, 'Target', 'testing')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bdec02-3d7a-49f8-8240-cb1039963ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(config: dict) -> None:\n",
    "    \"\"\"End-to-end data preparation pipeline with MLflow tracking\"\"\"\n",
    "    # Set experiment metadata\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": config[\"run_names\"][\"preprocessing\"],\n",
    "        \"dataset_type\": config[\"dataset\"][\"type\"],\n",
    "        \"task\": config[\"dataset\"][\"task\"]\n",
    "    })\n",
    "\n",
    "    # Log environment details\n",
    "    mlflow.log_params({\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"mlflow_version\": mlflow.__version__\n",
    "    })\n",
    "\n",
    "    eda_run_id = find_latest_run_id_by_experiment_and_stage(\n",
    "        config[\"experiment_names\"][\"eda\"],\n",
    "        config[\"run_names\"][\"eda\"]\n",
    "    )\n",
    "\n",
    "    data = get_dataset(eda_run_id, config[\"artifacts\"][\"data\"][\"raw\"])\n",
    "    log_data_artifacts(data)\n",
    "    \n",
    "    # Log dataset characteristics\n",
    "    mlflow.log_params({\n",
    "        \"num_samples\": data.shape[0],\n",
    "        \"original_features\": data.shape[1],\n",
    "        \"class_ratio\": data['Class'].value_counts().to_dict()[1]\n",
    "    })\n",
    "\n",
    "    # Split data\n",
    "    X = data.drop(columns=['Class', 'Time'])\n",
    "    y = data['Class']\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=config[\"dataset\"][\"split\"][\"val_size\"], stratify=y, random_state=config[\"dataset\"][\"split\"][\"random_state\"]\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=config[\"dataset\"][\"split\"][\"test_size\"], stratify=y_train_val, random_state=config[\"dataset\"][\"split\"][\"random_state\"]\n",
    "    )\n",
    "    # Process and log splits\n",
    "    datasets = prepare_datasets(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    log_split_metrics(datasets)\n",
    "    \n",
    "    # Data scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Log scaler as MLflow model\n",
    "    signature = infer_signature(X_train, scaler.transform(X_train))\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=scaler,\n",
    "        artifact_path=config[\"models\"][\"scaler\"][\"name\"],\n",
    "        signature=signature,\n",
    "        registered_model_name=config[\"models\"][\"scaler\"][\"registered_model_name\"]\n",
    "    )\n",
    "    \n",
    "    # Log processed datasets\n",
    "    scaled_datasets = prepare_datasets(\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "    )\n",
    "    log_processed_data(scaled_datasets, config[\"artifacts\"][\"data\"][\"processed\"])\n",
    "\n",
    "    # Add data validation checks\n",
    "    mlflow.log_metrics({\n",
    "        \"train_nan_count\": pd.DataFrame(X_train_scaled).isna().sum().sum(),\n",
    "        \"test_negative_samples\": (y_test == 1).sum()\n",
    "    })\n",
    "\n",
    "    # Log configuration\n",
    "    mlflow.log_dict(config, \"preprocessing_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7dce0e3-a2ef-4cde-854f-22b287789201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/06 18:07:52 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'CreditCardScaler' already exists. Creating a new version of this model...\n",
      "Created version '20' of model 'CreditCardScaler'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline completed. Run ID: 48285a75e94f47f58a1892a74f2b2226\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    experiment_name = CONFIG[\"experiment_names\"][\"preprocessing\"]\n",
    "    configure_mlflow(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"data_preprocessing\") as run:\n",
    "        data_pipeline(CONFIG)\n",
    "        print(\"Data pipeline completed. Run ID:\", mlflow.active_run().info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e3e7f-6c36-4017-ab0a-9d82522b33bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
