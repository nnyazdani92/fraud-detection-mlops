{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6022308f-3917-40a4-bfa7-19220d14652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.mlflow_utils import (\n",
    "    configure_mlflow, \n",
    "    load_config, \n",
    "    find_latest_run_id_by_experiment_and_stage, \n",
    "    get_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130f99b1-9c44-4364-9dc3-ddaf0c25a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488b95b5-f103-4043-a5f0-d92e6f01c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    \"\"\"Container for dataset splits with metadata\"\"\"\n",
    "    data: np.ndarray\n",
    "    name: str\n",
    "    data_type: str\n",
    "    split_type: str\n",
    "    features: list[str] = None\n",
    "    target: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ae55fd-c45b-4af4-9862-6561f31816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Class encapsulating data preprocessing functionality\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.raw_data = None\n",
    "        self.scaler = None\n",
    "        self.splits = {}\n",
    "        self._initialize_components()\n",
    "\n",
    "    def _initialize_components(self) -> None:\n",
    "        \"\"\"Initialize preprocessing components from config\"\"\"\n",
    "        self.scaler = StandardScaler()\n",
    "        self.test_size = self.config[\"dataset\"][\"split\"][\"test_size\"]\n",
    "        self.val_size = self.config[\"dataset\"][\"split\"][\"val_size\"]\n",
    "        self.random_state = self.config[\"dataset\"][\"split\"][\"random_state\"]\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Load and validate raw data from MLflow\"\"\"\n",
    "        eda_run_id = find_latest_run_id_by_experiment_and_stage(\n",
    "            self.config[\"experiment_names\"][\"eda\"],\n",
    "            self.config[\"run_names\"][\"eda\"]\n",
    "        )\n",
    "        self.raw_data = get_dataset(eda_run_id, self.config[\"artifacts\"][\"data\"][\"raw\"])\n",
    "        self._validate_raw_data()\n",
    "\n",
    "    def _validate_raw_data(self) -> None:\n",
    "        \"\"\"Perform initial data validation checks\"\"\"\n",
    "        if self.raw_data.empty:\n",
    "            raise ValueError(\"Loaded raw data is empty\")\n",
    "        if 'Class' not in self.raw_data.columns:\n",
    "            raise KeyError(\"Target column 'Class' missing in raw data\")\n",
    "        if self.raw_data.isnull().sum().sum() > 0:\n",
    "            raise ValueError(\"Raw data contains missing values\")\n",
    "\n",
    "    def split_data(self) -> None:\n",
    "        \"\"\"Perform stratified data splitting\"\"\"\n",
    "        X = self.raw_data.drop(columns=['Class', 'Time'])\n",
    "        y = self.raw_data['Class']\n",
    "        \n",
    "        # Initial train/test split\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.test_size,\n",
    "            stratify=y,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Secondary validation split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=self.val_size,\n",
    "            stratify=y_train_val,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Store splits\n",
    "        self.splits = {\n",
    "            'X_train': DatasetSplit(X_train, 'X_train', 'features', 'training'),\n",
    "            'X_val': DatasetSplit(X_val, 'X_val', 'features', 'validation'),\n",
    "            'X_test': DatasetSplit(X_test, 'X_test', 'features', 'testing'),\n",
    "            'y_train': DatasetSplit(y_train.values, 'y_train', 'target', 'training'),\n",
    "            'y_val': DatasetSplit(y_val.values, 'y_val', 'target', 'validation'),\n",
    "            'y_test': DatasetSplit(y_test.values, 'y_test', 'target', 'testing')\n",
    "        }\n",
    "\n",
    "    def scale_features(self) -> None:\n",
    "        \"\"\"Scale features and store transformed data\"\"\"\n",
    "        # Fit on training data\n",
    "        self.scaler.fit(self.splits['X_train'].data)\n",
    "        \n",
    "        # Transform all feature splits\n",
    "        for split in ['X_train', 'X_val', 'X_test']:\n",
    "            scaled_data = self.scaler.transform(self.splits[split].data)\n",
    "            self.splits[split].data = scaled_data\n",
    "\n",
    "    def validate_splits(self) -> None:\n",
    "        \"\"\"Validate split integrity and data quality\"\"\"\n",
    "        # Check data shapes\n",
    "        assert (self.splits['X_train'].data.shape[1] == \n",
    "                self.splits['X_test'].data.shape[1]), \"Feature dimension mismatch\"\n",
    "                \n",
    "        # Check for NaNs\n",
    "        for name, split in self.splits.items():\n",
    "            if np.isnan(split.data).any():\n",
    "                raise ValueError(f\"NaN values detected in {name}\")\n",
    "\n",
    "        # Check class distributions\n",
    "        train_pos = np.mean(self.splits['y_train'].data)\n",
    "        test_pos = np.mean(self.splits['y_test'].data)\n",
    "        if abs(train_pos - test_pos) > 0.05:\n",
    "            raise Warning(\"Significant class distribution shift between splits\")\n",
    "\n",
    "    def get_processed_data(self) -> dict[str, DatasetSplit]:\n",
    "        \"\"\"Return processed dataset splits\"\"\"\n",
    "        return self.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71da8db-edf3-4907-9495-6edeb68ab765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data_artifacts(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Log comprehensive data artifacts with schema\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # Log raw dataset sample\n",
    "        sample_path = Path(tmp_dir) / \"data_sample.csv\"\n",
    "        data.sample(1000).to_csv(sample_path, index=False)\n",
    "        mlflow.log_artifact(sample_path)\n",
    "        \n",
    "        # Log dataset schema\n",
    "        schema = pd.io.json.build_table_schema(data)\n",
    "        schema_path = Path(tmp_dir) / \"data_schema.json\"\n",
    "        schema_path.write_text(json.dumps(schema, indent=2))\n",
    "        mlflow.log_artifact(schema_path)\n",
    "\n",
    "def log_split_characteristics(splits: dict[str, DatasetSplit]) -> None:\n",
    "    \"\"\"Log dataset split metrics\"\"\"\n",
    "    metrics = {}\n",
    "    for name, split in splits.items():\n",
    "        if split.data_type == 'features':\n",
    "            metrics[f\"{name}_samples\"] = split.data.shape[0]\n",
    "            metrics[f\"{name}_features\"] = split.data.shape[1]\n",
    "        else:\n",
    "            metrics[f\"{name}_samples\"] = len(split.data)\n",
    "            \n",
    "        if split.data_type == 'target':\n",
    "            metrics[f\"{name}_positive\"] = np.sum(split.data)\n",
    "            metrics[f\"{name}_negative\"] = len(split.data) - np.sum(split.data)\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "def log_processed_splits(splits: dict[str, DatasetSplit], artifact_path: str) -> None:\n",
    "    \"\"\"Save and log processed datasets in efficient format\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        for name, split in splits.items():\n",
    "            file_path = Path(tmp_dir) / f\"{name}.parquet\"\n",
    "            pd.DataFrame(split.data).to_parquet(file_path)\n",
    "            mlflow.log_artifact(file_path, os.path.join(artifact_path, split.split_type))\n",
    "\n",
    "def log_scaler_model(scaler: StandardScaler, X_sample: pd.DataFrame, config: dict) -> None:\n",
    "    \"\"\"Log scaler as MLflow model with signature\"\"\"\n",
    "    signature = infer_signature(X_sample, scaler.transform(X_sample))\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=scaler,\n",
    "        artifact_path=config[\"models\"][\"scaler\"][\"name\"],\n",
    "        signature=signature,\n",
    "        registered_model_name=config[\"models\"][\"scaler\"][\"registered_model_name\"]\n",
    "    )\n",
    "    \n",
    "    # Log scaler parameters\n",
    "    mlflow.log_params({\n",
    "        \"scaler_mean\": json.dumps(scaler.mean_.tolist()),\n",
    "        \"scaler_scale\": json.dumps(scaler.scale_.tolist())\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6327466d-acbf-4665-803b-1986d3708720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(config: dict) -> None:\n",
    "    \"\"\"Main data preprocessing execution flow\"\"\"\n",
    "    # Initialize pipeline\n",
    "    preprocessor = DataPreprocessor(config)\n",
    "    \n",
    "    # Load and validate data\n",
    "    preprocessor.load_data()\n",
    "    log_data_artifacts(preprocessor.raw_data)\n",
    "    \n",
    "    # Split and process data\n",
    "    preprocessor.split_data()\n",
    "    preprocessor.scale_features()\n",
    "    preprocessor.validate_splits()\n",
    "    \n",
    "    # Log processed data\n",
    "    processed_splits = preprocessor.get_processed_data()\n",
    "    log_split_characteristics(processed_splits)\n",
    "    log_processed_splits(processed_splits, config[\"artifacts\"][\"data\"][\"processed\"])\n",
    "    \n",
    "    # Log scaler model\n",
    "    log_scaler_model(\n",
    "        preprocessor.scaler,\n",
    "        preprocessor.raw_data.drop(columns=['Class', 'Time']),\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # Log environment details\n",
    "    mlflow.log_params({\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"mlflow_version\": mlflow.__version__\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7dce0e3-a2ef-4cde-854f-22b287789201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/07 10:47:42 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Run ID: d3fa3913137d454f985b6dda41044f87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'CreditCardScaler' already exists. Creating a new version of this model...\n",
      "Created version '26' of model 'CreditCardScaler'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    experiment_name = CONFIG[\"experiment_names\"][\"preprocessing\"]\n",
    "    run_name = CONFIG[\"run_names\"][\"preprocessing\"]\n",
    "    \n",
    "    configure_mlflow(experiment_name)\n",
    "    \n",
    "    try:\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            mlflow.set_tags({\n",
    "                \"stage\": \"preprocessing\",\n",
    "                \"dataset_type\": CONFIG[\"dataset\"][\"type\"],\n",
    "                \"task\": CONFIG[\"dataset\"][\"task\"]\n",
    "            })\n",
    "            \n",
    "            mlflow.log_dict(CONFIG, \"preprocessing_config.json\")\n",
    "            preprocessing_pipeline(CONFIG)\n",
    "            mlflow.set_tag(\"status\", \"completed\")\n",
    "            \n",
    "            print(f\"Preprocessing completed. Run ID: {mlflow.active_run().info.run_id}\")\n",
    "    except Exception as e:\n",
    "        mlflow.log_param(\"error\", str(e))\n",
    "        mlflow.set_tag(\"status\", \"failed\")\n",
    "        mlflow.end_run()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e3e7f-6c36-4017-ab0a-9d82522b33bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
