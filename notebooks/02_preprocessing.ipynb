{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6022308f-3917-40a4-bfa7-19220d14652b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "sys.path.append('..')\n",
    "from dotenv import load_dotenv\n",
    "from src.mlflow_init import configure_mlflow\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130f99b1-9c44-4364-9dc3-ddaf0c25a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = os.environ[\"RAW_DATA_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71da8db-edf3-4907-9495-6edeb68ab765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data_artifacts(data: pd.DataFrame, artifact_path: str) -> None:\n",
    "    \"\"\"Log comprehensive data artifacts with schema\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # Log raw dataset sample\n",
    "        sample_path = os.path.join(tmp_dir, \"data_sample.csv\")\n",
    "        data.sample(1000).to_csv(sample_path, index=False)\n",
    "        mlflow.log_artifact(sample_path, artifact_path)\n",
    "        \n",
    "        # Log dataset schema\n",
    "        schema = pd.io.json.build_table_schema(data)\n",
    "        schema_path = os.path.join(tmp_dir, \"data_schema.json\")\n",
    "        with open(schema_path, \"w\") as outfile: \n",
    "            json.dump(schema, outfile, indent=2)\n",
    "            \n",
    "        mlflow.log_artifact(schema_path, artifact_path)\n",
    "\n",
    "def log_split_metrics(datasets: dict) -> None:\n",
    "    \"\"\"Log dataset metrics with provenance tracking\"\"\"\n",
    "    metrics = {}\n",
    "    for name, (data, _, _) in datasets.items():\n",
    "        metrics[f\"{name}_samples\"] = data.shape[0]\n",
    "        if len(data.shape) > 1:\n",
    "            metrics[f\"{name}_features\"] = data.shape[1]\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.15,\n",
    "        \"random_state\": 42,\n",
    "        \"stratify\": True\n",
    "    })\n",
    "\n",
    "def log_processed_data(datasets: dict, base_path: str) -> None:\n",
    "    \"\"\"Save and log processed datasets in efficient format\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        for name, (data, dtype, role) in datasets.items():\n",
    "            file_path = os.path.join(tmp_dir, f\"{name}.parquet\")\n",
    "            pd.DataFrame(data).to_parquet(file_path)\n",
    "            mlflow.log_artifact(file_path, os.path.join(base_path, role, dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6327466d-acbf-4665-803b-1986d3708720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(X_train, X_val, X_test, y_train, y_val, y_test) -> dict:\n",
    "    \"\"\"Create dataset dictionary with proper typing and metadata\"\"\"\n",
    "    return {\n",
    "        'X_train': (X_train, 'Features', 'training'),\n",
    "        'X_val': (X_val, 'Features', 'validation'),\n",
    "        'X_test': (X_test, 'Features', 'testing'),\n",
    "        'y_train': (y_train, 'Target', 'training'),\n",
    "        'y_val': (y_val, 'Target', 'validation'),\n",
    "        'y_test': (y_test, 'Target', 'testing')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1bdec02-3d7a-49f8-8240-cb1039963ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(raw_data_dir: str) -> None:\n",
    "    \"\"\"End-to-end data preparation pipeline with MLflow tracking\"\"\"\n",
    "    # Set experiment metadata\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"preprocessing\",\n",
    "        \"data_version\": \"1.0.0\",\n",
    "        \"dataset_type\": \"tabular\",\n",
    "        \"task\": \"classification\"\n",
    "    })\n",
    "    \n",
    "    # Load and log raw datarun\n",
    "    file_path = os.path.join(raw_data_dir, 'creditcard.csv')\n",
    "    data = pd.read_csv(file_path)\n",
    "    log_data_artifacts(data, \"data/raw\")\n",
    "    \n",
    "    # Log dataset characteristics\n",
    "    mlflow.log_params({\n",
    "        \"num_samples\": data.shape[0],\n",
    "        \"original_features\": data.shape[1],\n",
    "        \"class_ratio\": data['Class'].value_counts().to_dict()[1]\n",
    "    })\n",
    "\n",
    "    # Split data\n",
    "    X = data.drop(columns=['Class', 'Time'])\n",
    "    y = data['Class']\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42\n",
    "    )\n",
    "\n",
    "    # Process and log splits\n",
    "    datasets = prepare_datasets(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    log_split_metrics(datasets)\n",
    "    \n",
    "    # Data scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Log scaler as MLflow model\n",
    "    signature = infer_signature(X_train, scaler.transform(X_train))\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=scaler,\n",
    "        artifact_path=\"scaler\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"CreditCardScaler\"\n",
    "    )\n",
    "    \n",
    "    # Log processed datasets\n",
    "    scaled_datasets = prepare_datasets(\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "    )\n",
    "    log_processed_data(scaled_datasets, \"data/processed\")\n",
    "    \n",
    "    # Log environment details\n",
    "    mlflow.log_params({\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"mlflow_version\": mlflow.__version__\n",
    "    })\n",
    "\n",
    "    # Add data validation checks\n",
    "    mlflow.log_metrics({\n",
    "        \"train_nan_count\": pd.DataFrame(X_train_scaled).isna().sum().sum(),\n",
    "        \"test_negative_samples\": (y_test == 1).sum()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7dce0e3-a2ef-4cde-854f-22b287789201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/06 13:56:31 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'CreditCardScaler' already exists. Creating a new version of this model...\n",
      "Created version '14' of model 'CreditCardScaler'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpiuth4i_m/X_train.parquet data/processed/training/Features\n",
      "/tmp/tmpiuth4i_m/X_val.parquet data/processed/validation/Features\n",
      "/tmp/tmpiuth4i_m/X_test.parquet data/processed/testing/Features\n",
      "/tmp/tmpiuth4i_m/y_train.parquet data/processed/training/Target\n",
      "/tmp/tmpiuth4i_m/y_val.parquet data/processed/validation/Target\n",
      "/tmp/tmpiuth4i_m/y_test.parquet data/processed/testing/Target\n",
      "Data pipeline completed. Run ID: 6d7d2b94dd084dfe8c97692ef7b1414f\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    configure_mlflow(\"CreditCardFraudPreprocessing\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"data_preprocessing\") as run:\n",
    "        data_pipeline(RAW_DATA_DIR)\n",
    "        print(\"Data pipeline completed. Run ID:\", mlflow.active_run().info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e3e7f-6c36-4017-ab0a-9d82522b33bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
